{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Study: Enhancing Product Discovery for a Fashion Retailer with AI-Driven Image Classification\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "FashionTrend, a leading online fashion retailer, faces increasing challenges in managing its vast and diverse product catalog. As the product range grows, customers struggle to find items that match their preferences due to inconsistent categorization and limited filtering options. This results in a less-than-optimal shopping experience and potential loss in sales.\n",
    "\n",
    "To address this, FashionTrend aims to streamline product categorization by developing an AI-based image classification system to automatically categorize each new product image. This solution intends to:\n",
    "\n",
    "- **Enhance Search and Filtering**: Improve customers' ability to find items quickly and accurately by organizing products into detailed categories such as gender, category (e.g., apparel, accessories), and type (e.g., shirts, jeans, watches).\n",
    "- **Optimize Inventory Management**: Support the operations team by ensuring each item is categorized correctly, making it easier to track and restock products according to demand.\n",
    "\n",
    "### Project Approach\n",
    "\n",
    "#### Objective\n",
    "The primary objective of this project is to build a robust machine learning model that categorizes products based on images across multiple attributes, including `masterCategory`,`Gender`.Automating this labeling process will improve product discoverability and inventory management efficiency.\n",
    "\n",
    "#### Dataset Overview\n",
    "\n",
    "1. **Styles.csv**: Contains metadata for each product, including attributes like gender, masterCategory, subCategory, articleType, baseColour, season, year, usage, and productDisplayName.\n",
    "2. **Images**: High-quality product images labeled with IDs that correspond to entries in `styles.csv`.\n",
    "\n",
    "#### Proposed Steps\n",
    "\n",
    "1. **Data Exploration and Preprocessing**:\n",
    "   - **Inspect Metadata**: Analyze `styles.csv` to understand the distribution of categories and prepare labels.\n",
    "   - **Image Preparation**: Resize and standardize images for model training, matching each image to its corresponding metadata in `styles.csv`.\n",
    "\n",
    "2. **Building the Model**:\n",
    "   - **Image Classification Model**: Develop a convolutional neural network (CNN) to classify images by multiple attributes. Begin with `masterCategory` (e.g., Apparel, Accessories) and expand to `gender` as multi-label classification tasks.\n",
    "   - **Multi-Label Classification**: Use multi-label classification techniques to predict multiple attributes for each image, making the model versatile for complex queries.\n",
    "\n",
    "3. **Model Evaluation and Optimization**:\n",
    "   - **Accuracy Metrics**: Evaluate model performance using metrics such as accuracy, precision\n",
    "\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set seeds for reproducibilitys\n",
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Importing the dependencies\n",
    "\n",
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "from zipfile import ZipFile\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaggle key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    kaggle_credentials = json.load(open(\"kaggle.json\"))\n",
    "    os.environ['KAGGLE_USERNAME'] = kaggle_credentials[\"username\"]\n",
    "    os.environ['KAGGLE_KEY'] = kaggle_credentials[\"key\"]\n",
    "except FileNotFoundError:\n",
    "    print(\"kaggle.json file not found. Please place it in the directory.\")\n",
    "except KeyError:\n",
    "    print(\"Error: Invalid JSON format in kaggle.json.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kaggle\n",
    "from zipfile import ZipFile\n",
    "\n",
    "# Define dataset name and zip file name\n",
    "dataset_name = 'fashionData'\n",
    "zip_file_name = 'fashionData.zip'\n",
    "\n",
    "# Check if the dataset folder exists\n",
    "if not os.path.exists(dataset_name): \n",
    "    # Check if the zip file exists\n",
    "    if not os.path.exists(zip_file_name):\n",
    "        # Use Kaggle API to download the dataset\n",
    "        kaggle.api.dataset_download_files(\"bhavikjikadara/e-commerce-products-images\", path='.', unzip=False)\n",
    "        \n",
    "    # Extract the dataset if the directory doesn't exist\n",
    "    with ZipFile(zip_file_name, 'r') as zip_ref:\n",
    "        zip_ref.extractall(dataset_name)\n",
    "\n",
    "print(f\"Dataset is ready and extracted in '{dataset_name}' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "styles = pd.read_csv(r\"C:\\Users\\pascal\\Desktop\\New Projects\\AI-Driven-Image-Classification-for-Fashion-Retailer\\fashionData\\styles.csv\")\n",
    "styles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "styles.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "styles.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = styles.duplicated().sum()\n",
    "duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated color mapping dictionary based on unique baseColour values\n",
    "color_mapping = {\n",
    "    'Navy Blue': 'Blue', 'Blue': 'Blue', 'Turquoise Blue': 'Blue', 'Teal': 'Blue', 'Steel': 'Gray',\n",
    "    'Silver': 'Gray', 'Grey': 'Gray', 'Charcoal': 'Gray', 'Grey Melange': 'Gray',\n",
    "    'Green': 'Green', 'Olive': 'Green', 'Lime Green': 'Green', 'Sea Green': 'Green', 'Fluorescent Green': 'Green',\n",
    "    'Purple': 'Purple', 'Lavender': 'Purple', 'Mauve': 'Purple', 'Magenta': 'Purple',\n",
    "    'Black': 'Black', 'White': 'White', 'Off White': 'White', 'Cream': 'White',\n",
    "    'Beige': 'Beige', 'Khaki': 'Beige', 'Skin': 'Beige', 'Taupe': 'Beige',\n",
    "    'Brown': 'Brown', 'Coffee Brown': 'Brown', 'Mushroom Brown': 'Brown', 'Tan': 'Brown', 'Bronze': 'Brown', 'Copper': 'Brown',\n",
    "    'Red': 'Red', 'Maroon': 'Red', 'Burgundy': 'Red', 'Rose': 'Pink', 'Pink': 'Pink',\n",
    "    'Yellow': 'Yellow', 'Mustard': 'Yellow', 'Gold': 'Yellow',\n",
    "    'Orange': 'Orange', 'Peach': 'Orange', 'Rust': 'Orange',\n",
    "    'Multi': 'Multi', 'Metallic': 'Multi', 'Nude': 'Other'\n",
    "}\n",
    "\n",
    "# Apply the mapping to create a new column for the broader color categories\n",
    "styles['mainColor'] = styles['baseColour'].map(color_mapping).fillna('Other')\n",
    "\n",
    "# Check the unique values in the new mainColor column to verify the mapping\n",
    "print(styles['mainColor'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def explore_selected_columns(df):\n",
    "    # List of columns to explore\n",
    "    columns_to_explore = ['masterCategory', 'gender', 'mainColor', 'usage', 'season']\n",
    "    \n",
    "    # Loop through each selected column\n",
    "    for column in columns_to_explore:\n",
    "        if column in df.columns:\n",
    "            # Check if the column is of type object (string/categorical data)\n",
    "            if df[column].dtype == 'object':\n",
    "                print(f\"Column: {column}\")\n",
    "                print(\"Unique Values:\", df[column].unique())\n",
    "                print(\"\\n\")\n",
    "                \n",
    "                # Plot distribution for categorical columns\n",
    "                plt.figure(figsize=(8, 4))\n",
    "                sns.countplot(y=column, data=df, order=df[column].value_counts().index)\n",
    "                plt.title(f\"Distribution of {column}\")\n",
    "                plt.show()\n",
    "            \n",
    "            # If the column is numerical, plot a histogram\n",
    "            elif df[column].dtype in ['int64', 'float64']:\n",
    "                print(f\"Column: {column}\")\n",
    "                print(\"Statistical Summary:\")\n",
    "                print(df[column].describe())\n",
    "                print(\"\\n\")\n",
    "                \n",
    "                # Plot distribution for numerical columns\n",
    "                plt.figure(figsize=(8, 4))\n",
    "                sns.histplot(df[column], kde=True)\n",
    "                plt.title(f\"Distribution of {column}\")\n",
    "                plt.xlabel(column)\n",
    "                plt.show()\n",
    "\n",
    "# Usage\n",
    "explore_selected_columns(styles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Directory path for images\n",
    "image_dir = r\"C:\\Users\\pascal\\Desktop\\New Projects\\AI-Driven-Image-Classification-for-Fashion-Retailer\\fashionData\\e-commerce\\images\"\n",
    "\n",
    "# Function to display 2 images from each category, with a check for available images\n",
    "def visualize_two_samples_per_category(styles, image_dir, samples_per_category=2):\n",
    "    categories = styles['masterCategory'].unique()\n",
    "    \n",
    "    # Set up a compact plot grid\n",
    "    fig, axes = plt.subplots(len(categories), samples_per_category, figsize=(6, len(categories) * 1.8))\n",
    "    fig.suptitle('Two Samples from Each Category', fontsize=10)\n",
    "\n",
    "    for i, category in enumerate(categories):\n",
    "        # Get valid image IDs for the current category, up to the required number\n",
    "        category_images = styles[styles['masterCategory'] == category]['id'].tolist()\n",
    "        valid_images = [img_id for img_id in category_images if os.path.exists(os.path.join(image_dir, f\"{img_id}.jpg\"))][:samples_per_category]\n",
    "\n",
    "        for j, img_id in enumerate(valid_images):\n",
    "            img_path = os.path.join(image_dir, f\"{img_id}.jpg\")\n",
    "            img = mpimg.imread(img_path)\n",
    "            axes[i, j].imshow(img)\n",
    "            axes[i, j].axis('off')\n",
    "            if j == 0:\n",
    "                axes[i, j].set_title(category, loc='left', fontsize=10)\n",
    "\n",
    "        # Turn off axes for any unused spots in the row (if fewer images than samples_per_category)\n",
    "        for j in range(len(valid_images), samples_per_category):\n",
    "            axes[i, j].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.92, hspace=0.4)  # Adjust title position and spacing\n",
    "    plt.show()\n",
    "\n",
    "# Call the function to display 2 samples per category\n",
    "visualize_two_samples_per_category(styles, image_dir)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create a simplified multi_label column with only masterCategory and gender\n",
    "styles['multi_label'] = styles[['masterCategory', 'gender']].apply(lambda x: '_'.join(x.astype(str)), axis=1)\n",
    "\n",
    "# Step 2: Define the categories to merge into \"Other_Unisex\"\n",
    "categories_to_merge = ['Sporting Goods_Unisex', 'Personal Care_Unisex', 'Free Items_Unisex', 'Home_Unisex']\n",
    "\n",
    "# Step 3: Update the multi_label column, replacing specific categories with \"Other_Unisex\"\n",
    "styles['multi_label'] = styles['multi_label'].apply(lambda x: 'Other_Unisex' if x in categories_to_merge else x)\n",
    "\n",
    "# Step 4: Display unique names and their counts to confirm the final class distribution\n",
    "unique_counts = styles['multi_label'].value_counts()\n",
    "print(\"Updated unique names and their counts:\")\n",
    "print(unique_counts)\n",
    "\n",
    "# Step 5: Verify the total number of unique classes in multi_label\n",
    "print(\"\\nTotal unique classes in 'multi_label':\", unique_counts.nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Ensure 'id' column is formatted correctly with '.jpg' extension for image filenames\n",
    "styles['id'] = styles['id'].astype(str) + \".jpg\"  # Append .jpg to match image file names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image setup, Data Processing & Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Step 2: Data Processing and Train-Test Split\n",
    "\n",
    "# Define image size and batch size for efficient processing\n",
    "img_size = 128\n",
    "batch_size = 32\n",
    "\n",
    "# Step 3: Initialize ImageDataGenerator with augmentations for training and validation sets\n",
    "# The augmentations will help improve model generalization by applying small variations to the training images.\n",
    "data_gen = ImageDataGenerator(\n",
    "    rescale=1. / 255,            # Normalize pixel values between 0 and 1\n",
    "    rotation_range=20,           # Increase rotation to 30 degrees for more variability\n",
    "    width_shift_range=0.1,       # Increase horizontal shift to 20%\n",
    "    height_shift_range=0.1,      # Increase vertical shift to 20%\n",
    "    zoom_range=0.1,              # Increase zoom range to 20%\n",
    "    shear_range=0.1,             # Add shear transformations\n",
    "    horizontal_flip=True,        # Flip images horizontally\n",
    "    validation_split=0.2         # Reserve 20% of data for validation\n",
    ")\n",
    "\n",
    "# Step 4: Configure the training data generator\n",
    "# This generator will create batches of augmented images for training.\n",
    "train_generator = data_gen.flow_from_dataframe(\n",
    "    dataframe=styles,            # Data source: styles DataFrame containing image IDs and labels\n",
    "    directory=image_dir,         # Directory where images are stored\n",
    "    x_col=\"id\",                  # Column name for image filenames in styles DataFrame\n",
    "    y_col=\"multi_label\",         # Column name for labels in styles DataFrame\n",
    "    target_size=(img_size, img_size),  # Resize images to a consistent size\n",
    "    batch_size=batch_size,       # Number of images per batch\n",
    "    class_mode='categorical',    # Treat each unique label as a separate class\n",
    "    subset='training',           # Use the training subset\n",
    "    seed=42                      # Seed for reproducibility\n",
    ")\n",
    "\n",
    "# Step 5: Configure the validation data generator\n",
    "# This generator will provide batches of images for validation, without additional augmentation.\n",
    "validation_generator = data_gen.flow_from_dataframe(\n",
    "    dataframe=styles,            # Data source: styles DataFrame\n",
    "    directory=image_dir,         # Directory containing the images\n",
    "    x_col=\"id\",                  # Image filenames column\n",
    "    y_col=\"multi_label\",         # Label column\n",
    "    target_size=(img_size, img_size),  # Consistent resizing for validation images\n",
    "    batch_size=batch_size,       # Batch size\n",
    "    class_mode='categorical',    # Multi-class setup for categorical labels\n",
    "    subset='validation',         # Use the validation subset\n",
    "    seed=42                      # Seed for reproducibility\n",
    ")\n",
    "\n",
    "# Confirmation of setup completion\n",
    "print(\"Train generator setup complete.\")\n",
    "print(\"Validation generator setup complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cnn Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Define a deeper sequential model\n",
    "model = models.Sequential()\n",
    "\n",
    "# First Convolutional Block\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same',input_shape=(img_size, img_size, 3)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D(2, 2))\n",
    "\n",
    "# Second Convolutional Block\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D(2, 2))\n",
    "\n",
    "# Third Convolutional Block\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D(2, 2))\n",
    "\n",
    "# Fourth Convolutional Block\n",
    "model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D(2, 2))\n",
    "\n",
    "# Flatten layer to convert 3D feature maps to 1D feature vectors\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "# Fully Connected Layers with Dropout for regularization\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "# Output Layer: Softmax activation for multi-class classification\n",
    "num_classes = len(train_generator.class_indices)\n",
    "model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Displaying the model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with Adam optimizer and categorical crossentropy loss\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001),  # Adaptive optimizer with a learning rate of 0.0001\n",
    "              loss='categorical_crossentropy',       # Loss function for multi-class classification\n",
    "              metrics=['accuracy'])                  # Metric to evaluate performance during training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks for training\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True),  # Stop if validation loss doesn't improve for 7 epochs, restoring best weights\n",
    "    ModelCheckpoint('best_model.keras', save_best_only=True)  # Save only the best model during training\n",
    "]\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,                                 # Training data generator\n",
    "    steps_per_epoch=train_generator.samples //batch_size,  # Steps per epoch based on dataset size and batch size\n",
    "    epochs=10,                                       # Number of epochs to train\n",
    "    validation_data=validation_generator,            # Validation data generator\n",
    "    validation_steps=validation_generator.samples // batch_size,  # Validation steps per epoch\n",
    "    callbacks=callbacks                  # Apply callbacks for early stopping and best model saving\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation on the validation set\n",
    "\n",
    "print(\"Evaluating Cnn model...\")\n",
    "val_loss, val_accuracy = model.evaluate(validation_generator, steps=validation_generator.samples // batch_size)  # Evaluate on validation data\n",
    "print(f\"Validation Accuracy: {val_accuracy * 100:.2f}%\")  # Print validation accuracy in percentage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAve model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save the model locally\n",
    "\n",
    "model.save('cnn_fashion_model.h5')\n",
    "model.save('cnn_fashion_model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acuracy Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values across epochs\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values across epochs\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Mapping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping class indices to class names\n",
    "class_indices = {v: k for k, v in train_generator.class_indices.items()}  # Create class mapping\n",
    "\n",
    "# Save class indices to JSON\n",
    "with open('class_indices.json', 'w') as f:\n",
    "    json.dump(class_indices, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Image Preprocessing pipeline and Prediction Test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "TARGET_SIZE = (128, 128)  \n",
    "\n",
    "# Load and preprocess a single image\n",
    "def load_and_preprocess_image(image_path, target_size=TARGET_SIZE):\n",
    "    \"\"\"\n",
    "    Load an image, resize it to target size, convert to numpy array, \n",
    "    add batch dimension, and normalize pixel values.\n",
    "    \"\"\"\n",
    "    # Load the image with Pillow\n",
    "    img = Image.open(image_path)\n",
    "    # Resize to target size\n",
    "    img = img.resize(target_size)\n",
    "    # Convert to numpy array\n",
    "    img_array = np.array(img)\n",
    "    # Expand dimensions to match model input shape (1, 128, 128, 3)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    # Normalize pixel values\n",
    "    img_array = img_array.astype('float32') / 255.\n",
    "    return img_array\n",
    "\n",
    "# Function to predict class for a single image using CNN model\n",
    "def predict_image_class_cnn(image_path, class_indices):\n",
    "    \"\"\"\n",
    "    Predict the class of a preprocessed image using the trained CNN model.\n",
    "    \"\"\"\n",
    "    # Load the CNN model\n",
    "    cnn_model = load_model('cnn_fashion_model.keras')\n",
    "    # Preprocess the input image\n",
    "    preprocessed_img = load_and_preprocess_image(image_path)\n",
    "    # Predict with the CNN model\n",
    "    predictions = cnn_model.predict(preprocessed_img)\n",
    "    # Get the index of the class with the highest prediction score\n",
    "    predicted_class_index = np.argmax(predictions, axis=1)[0]\n",
    "    # Convert the class index back to the class name\n",
    "    predicted_class_name = class_indices[predicted_class_index]\n",
    "    return predicted_class_name\n",
    "\n",
    "# Example usage for CNN model\n",
    "test_image_path = r\"C:\\Users\\pascal\\Desktop\\New Projects\\AI-Driven-Image-Classification-for-Fashion-Retailer\\fashionData\\e-commerce\\images\\1536.jpg\"\n",
    "predicted_class_cnn = predict_image_class_cnn(test_image_path, class_indices)\n",
    "print(\"Predicted Class using CNN model:\", predicted_class_cnn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MobileNetV2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set input image size for MobileNetV2 (recommended size for this architecture)\n",
    "img_size = 128\n",
    "\n",
    "# Load the MobileNetV2 model with pre-trained weights from ImageNet\n",
    "# Exclude the top layers, as we'll add custom layers for our classification task\n",
    "base_model = MobileNetV2(input_shape=(img_size, img_size, 3), include_top=False, weights='imagenet')\n",
    "\n",
    "# Freeze the base layers to retain pre-trained weights\n",
    "base_model.trainable = False\n",
    "\n",
    "# Define the model\n",
    "model = models.Sequential([\n",
    "    base_model,  # Use MobileNetV2 as the base\n",
    "    layers.GlobalAveragePooling2D(),  # Global average pooling to reduce dimensions\n",
    "    layers.Dense(256, activation='relu'),  # Dense layer with ReLU activation\n",
    "    layers.Dropout(0.5),  # Dropout layer for regularization\n",
    "    layers.Dense(len(train_generator.class_indices), activation='softmax')  # Output layer with softmax for classification\n",
    "])\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks for early stopping and model checkpointing\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True),\n",
    "    ModelCheckpoint('best_mobilenetv2_model.keras', save_best_only=True)\n",
    "]\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    epochs=10,  \n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // batch_size,\n",
    "    callbacks=callbacks\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Model Evaluation\n",
    "print(\"Evaluating MobileNetV2 model...\")\n",
    "val_loss, val_accuracy = model.evaluate(validation_generator, steps=validation_generator.samples // batch_size)\n",
    "print(f\"MobileNetV2 Validation Accuracy: {val_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save the model locally\n",
    "\n",
    "model.save('mobilenetv2_model.h5')\n",
    "model.save('mobilenetv2_model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acuracy Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values across epochs\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values across epochs\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping class indices to class names\n",
    "class_indices = {v: k for k, v in train_generator.class_indices.items()}  # Create class mapping\n",
    "\n",
    "# Save class indices to JSON\n",
    "with open('class_indices_mobilenetv2.json', 'w') as f:\n",
    "    json.dump(class_indices, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image processing and predicting Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define constants\n",
    "TARGET_SIZE = (128, 128)  # Ensure this matches the input size used for training\n",
    "\n",
    "# Load and preprocess a single image\n",
    "def load_and_preprocess_image(image_path, target_size=TARGET_SIZE):\n",
    "    \"\"\"\n",
    "    Load an image, resize it to target size, convert to numpy array, \n",
    "    add batch dimension, and normalize pixel values.\n",
    "    \"\"\"\n",
    "    # Load the image with Pillow\n",
    "    img = Image.open(image_path)\n",
    "    # Resize to target size\n",
    "    img = img.resize(target_size)\n",
    "    # Convert to numpy array\n",
    "    img_array = np.array(img)\n",
    "    # Expand dimensions to match model input shape (1, 128, 128, 3)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    # Normalize pixel values\n",
    "    img_array = img_array.astype('float32') / 255.\n",
    "    return img_array\n",
    "\n",
    "# Function to predict class for a single image using MobileNetV2 model\n",
    "def predict_image_class_mobilenetv2(image_path, class_indices):\n",
    "    \"\"\"\n",
    "    Predict the class of a preprocessed image using the trained MobileNetV2 model.\n",
    "    \"\"\"\n",
    "    # Load the MobileNetV2 model\n",
    "    mobilenetv2_model = load_model('mobilenetv2_model.keras')\n",
    "    # Preprocess the input image\n",
    "    preprocessed_img = load_and_preprocess_image(image_path)\n",
    "    # Predict with the MobileNetV2 model\n",
    "    predictions = mobilenetv2_model.predict(preprocessed_img)\n",
    "    # Get the index of the class with the highest prediction score\n",
    "    predicted_class_index = np.argmax(predictions, axis=1)[0]\n",
    "    # Convert the class index back to the class name\n",
    "    predicted_class_name = class_indices[predicted_class_index]\n",
    "    return predicted_class_name\n",
    "\n",
    "# Example usage for MobileNetV2 model\n",
    "test_image_path = r\"C:\\Users\\pascal\\Desktop\\New Projects\\AI-Driven-Image-Classification-for-Fashion-Retailer\\fashionData\\e-commerce\\images\\1536.jpg\"\n",
    "predicted_class_mobilenet = predict_image_class_mobilenetv2(test_image_path, class_indices)\n",
    "print(\"Predicted Class using MobileNetV2 model:\", predicted_class_mobilenet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
